# BPE

## question2

### 问题和我的回答

(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than
UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various
input strings.
因为把一个一个的token编码为utf-8更省内存，序列更短，模型训练，推理的长度也更短。此外，现在绝大部分的句子都是英文，在utf-8下可以编码为一个字节，只有少部分的token需要多个字节。
(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into
a Unicode string. Why is this function incorrect? Provide an example of an input byte string
that yields incorrect results.
```python
def decode_utf8_bytes_to_str_wrong(bytestring: bytes):
    return "".join([bytes([b]).decode("utf-8") for b in bytestring])
>>> decode_utf8_bytes_to_str_wrong("hello".encode("utf-8"))
'hello'
```
因为函数是把每一个字节给单独decode出来，但是除了英文字符以外，绝大多数的unicode在utf-8编码下会有多个字节，其解码也是需要多个字节才能解码的，如果是"我很帅",改函数就无法正确把其utf-8字节转化为原始字节。
(c) Give a two byte sequence that does not decode to any Unicode character(s).
utf-8的编码是有规律的：
1. 一个字节表示的需要用0开头
2. 两个字节的表示的，第一个是110开头，第二个是10开头
3. 三个字节表示的，第一个是1110开头，后面都是10开头
4. 四个字节表示的，第一个是11110开头，后面都是10开头
只要不符合上面的规律，就无法使用uft-8进行解码，比如说字节"1000000 1000000"就无法使用utf-8解码为unicode字符

### 综合评价

  * **优点:**

      * **(a) UTF-8的优势:** 您正确地指出了UTF-8在内存效率和序列长度上的优势，特别是对于以英文为主的文本。
      * **(b) 错误的解码函数:** 您准确地分析出问题在于逐字节（byte-by-byte）解码，并意识到这对于多字节字符是行不通的。
      * **(c) 无效的UTF-8序列:** 您正确地描述了UTF-8的编码规则，并基于这些规则给出了一个有效的反例。

  * **可完善之处:**

      * 可以更深入地解释为什么UTF-8的这些特性对BPE分词器训练至关重要。
      * 可以提供更具体的代码示例来展示错误解码函数的失败，并与正确的方式进行对比。
      * 可以对无效字节序列的解释更精确一些，并给出更多例子。

-----

### 详细知识整理

接下来，我们把这些知识点串联起来，形成一个完整的知识图谱。

#### 1. Unicode: 统一的字符集标准

首先，最核心的概念是 **Unicode**。

  * **目标:** Unicode的目标是为世界上每一个字符提供一个唯一的数字编号，这个编号称为“码点”（Code Point）。例如，字母 `A` 的码点是 `U+0041`，汉字 “帅” 的码点是 `U+5E05`。
  * **它不是编码:** Unicode本身只是一张巨大的“字符-数字”对应表，它不规定这些数字在计算机中如何存储为字节。你可以把它想象成一本字典，只定义了每个字是第几页第几个，但没说这本书本身应该用什么纸张和油墨来印刷。因此码点无法直接保存到计算机上或者通过网络进行传输。

#### 2. UTF-8, UTF-16, UTF-32: Unicode的实现方式 (编码)

为了在计算机上存储和传输Unicode码点，我们需要具体的编码方案（Encoding Scheme）。UTF-8, UTF-16, 和 UTF-32 就是最常见的三种方案。

  * **UTF-32 (定长编码):**

      * **原理:** 最简单直接，每个Unicode码点都使用固定的4个字节（32位）来存储。
      * **优点:** 查找和处理单个字符非常快，因为每个字符长度都一样。
      * **缺点:** 空间浪费严重。例如，存储英文文本 `"hello"`，每个字母都只占1个字节就够了，但UTF-32会用4个字节来存，浪费了75%的空间。

  * **UTF-16 (变长编码):**

      * **原理:** 对常用的字符（基本多文种平面，BMP）使用2个字节存储，对于超出这个范围的生僻字、Emoji等，使用4个字节（通过一种叫做“代理对”的机制）。
      * **优点:** 相对于UTF-32，节省了大量空间。
      * **缺点:** 仍然是变长编码，处理起来比UTF-32复杂。对于纯英文文本，空间效率不如UTF-8。

  * **UTF-8 (变长编码):**

      * **原理:** 一种高度优化的变长编码方案。它根据码点的大小，使用1到4个字节来表示一个字符。
          * **ASCII 兼容:** 对于所有英文字母、数字和常见符号（码点 `U+0000` 到 `U+007F`），UTF-8只使用1个字节表示，并且其字节值与ASCII编码完全相同。
          * **多字节表示:** 对于其他字符，它使用2、3或4个字节，并通过每个字节头部的特定位模式来区分。

-----

#### 3. 问题 (a) 深入解析：为何BPE分词器偏爱在UTF-8字节上训练？

您的回答已经很好了，我们来把它和BPE（Byte Pair Encoding）分词联系起来。

BPE的目标是找到一种高效的方式来切分词汇，既能处理常见词，也能通过组合来表示未知词（Out-of-Vocabulary, OOV）。

**为什么选择在UTF-8字节上训练，而不是字符或UTF-16/32？**

1.  **空间效率与序列长度（您的回答核心）:**

      * **更短的序列:** 正如您所说，现代文本数据（尤其是互联网语料）中，英文和数字占了很大比重。使用UTF-8，这些字符都只占1个字节。这意味着编码后的**字节序列**会比UTF-16或UTF-32短得多。在Transformer等模型中，处理序列的计算成本与序列长度的平方成正比（$O(n^2)$），因此更短的序列意味着更快的训练和推理速度。
      * **对比举例:**
          * 字符串: `"hello帥"`
          * UTF-8 字节: `b'hello\xe5\xb8\x85'` (长度为 5 + 3 = 8)
          * UTF-16 字节 (Little Endian): `b'h\x00e\x00l\x00l\x00o\x00\x05^'` (长度为 5*2 + 2 = 12)
          * UTF-32 字节 (Little Endian): `b'h\x00\x00\x00e\x00\x00\x00l\x00\x00\x00l\x00\x00\x00o\x00\x00\x00\x05^\x00\x00'` (长度为 5*4 + 4 = 24)
            很明显，UTF-8产生的字节序列最短。

2.  **统一的词汇空间，彻底消除OOV:**

      * 在原始字符级别上训练BPE，词汇表就是所有见过的字符。如果遇到一个训练时没见过的字符（比如一个新的Emoji），它就成了OOV。
      * 但是，**任何Unicode字符都可以被UTF-8编码成一个字节序列**。UTF-8的编码空间只有256个可能的字节值（从 `0x00` 到 `0xFF`）。
      * 通过在字节级别上运行BPE，**初始词汇表就是这256个字节**。任何字符串，无论包含多么生僻的字符，都可以被表示为这个基础词汇表的序列。这意味着**理论上不存在OOV问题**。模型最终学会的是将字节组合成有意义的子词（subword），而不是处理字符。

3.  **跨语言的一致性:**

      * 不同语言的字符集大小差异巨大。如果基于字符训练，就需要为不同语言设计不同的词汇表。而基于UTF-8字节，所有语言都被统一到了同一个256大小的基础字节词汇表中，使得构建多语言模型变得更加简单。

**结论:** 在UTF-8字节上训练BPE，不仅因为其存储和计算效率高，更因为它提供了一个有限且完备的词汇空间（256个字节），从根本上解决了OOV问题，并为构建多语言模型提供了优雅的统一框架。

-----

#### 4. 问题 (b) 深入解析：错误的UTF-8解码

您的回答完全正确。核心问题在于 **UTF-8的单个字节不一定能独立构成一个完整的字符**。

  * **Python中的`bytes`和`str`:**

      * `str`: 是一个Unicode字符序列。`len("我很帅")` 的结果是3，因为它有3个字符。
      * `bytes`: 是一个字节序列。`len("我很帅".encode("utf-8"))` 的结果是9，因为它被编码成了9个字节。

  * **代码分析:**

    ```python
    def decode_utf8_bytes_to_str_wrong(bytestring: bytes):
        # 错误的做法：对字节序列进行迭代，并尝试单独解码每个字节
        # b 会依次成为 bytestring 中的每一个整数值（0-255）
        return "".join([bytes([b]).decode("utf-8") for b in bytestring])
    ```

    这个函数的逻辑是“把一个字节序列拆成N个只有一个字节的序列，然后分别对这N个序列进行UTF-8解码”。

  * **失败的例子（您的例子就很完美）:**
    我们来实际运行一下：

    ```python
    >>> text = "我很帅"
    >>> utf8_bytes = text.encode("utf-8")
    >>> print(utf8_bytes)
    b'\xe6\x88\x91\xe5\xbe\x88\xe5\xb8\x85'

    >>> # 正确的解码方式：对整个字节序列调用 .decode()
    >>> utf8_bytes.decode("utf-8")
    '我很帅'

    >>> # 错误的解码方式
    >>> try:
    ...     decode_utf8_bytes_to_str_wrong(utf8_bytes)
    ... except UnicodeDecodeError as e:
    ...     print(e)
    ...
    'utf-8' codec can't decode byte 0xe6 in position 0: invalid continuation byte
    ```

    **为什么会报错？**

    1.  循环的第一个字节是 `0xe6` (二进制 `11100110`)。
    2.  UTF-8解码器看到这个字节以 `1110` 开头，它会说：“好的，这是一个三字节字符的开始，我需要**后面紧跟着两个以`10`开头**的字节才能完成解码。”
    3.  但是，`decode_utf8_bytes_to_str_wrong` 函数只给了它 `bytes([0xe6])` 这一个字节，后面没有了。解码器等不到它需要的后续字节，于是抛出 `UnicodeDecodeError` 异常。

-----

#### 5. 问题 (c) 深入解析：构造无效的UTF-8字节序列

您的回答指明了正确的方向：违反UTF-8的编码结构规则。

**UTF-8 编码规则总结:**

| 字节数 | 第1字节 (二进制) | 第2字节 (二进制) | 第3字节 (二进制) | 第4字节 (二进制) |
| :--- | :--- | :--- | :--- | :--- |
| 1 | `0xxxxxxx` | | | |
| 2 | `110xxxxx` | `10xxxxxx` | | |
| 3 | `1110xxxx` | `10xxxxxx` | `10xxxxxx` | |
| 4 | `11110xxx` | `10xxxxxx` | `10xxxxxx` | `10xxxxxx` |

  * `0xxxxxxx`: 称为**起始字节 (Start Byte)**，同时也是单字节字符。
  * `110xxxxx`, `1110xxxx`, `11110xxx`: 称为**起始字节**，它们宣告了后面需要跟随多少个延续字节。
  * `10xxxxxx`: 称为**延续字节 (Continuation Byte)**。它**不能**作为一个字符的开头。

基于这些规则，构造无效序列就很容易了。

  * **您的例子:** `"1000000 1000000"` (应写为 `b'\x80\x80'`)

      * **分析:** 这是一个完美的例子。第一个字节 `10000000` (即 `0x80`) 是一个延续字节。解码器在序列的开头看到了一个延续字节，它会立即报错，因为它期望看到的是一个起始字节（以`0`或`11`开头）。

  * **其他无效的例子:**

    1.  **起始字节后面没有足够的延续字节:**

          * `b'\xe6\x88'` (二进制 `11100110 10001000`)
          * **分析:** `0xe6` ( `11100110` ) 是一个三字节字符的起始字节，解码器期望后面有两个延续字节，但这里只给了一个。

    2.  **起始字节后面跟了一个非延续字节:**

          * `b'\xc2\x41'` (二进制 `11000010 01000001`)
          * **分析:** `0xc2` (`11000010`) 是一个两字节字符的起始字节，解码器期望后面跟一个延续字节（以`10`开头）。但 `0x41` (字母`'A'`) 的二进制是 `01000001`，它是一个单字节字符，不是延续字节。

    3.  **过长编码 (Overlong Encodings):**

          * `b'\xc0\x80'` (二进制 `11000000 10000000`)
          * **分析:** 这在技术上解码出的码点是 `U+0000`，但 `U+0000` 已经规定了必须用单字节 `b'\x00'` 来表示。出于安全原因，这种可以用更短字节序列表示的字符的过长形式被视为无效。

### Subword Tokenization

#### 1. 分词的困境：从词到字节的两难选择

在构建语言模型时，我们首先需要将文本分割成一个个小单元，这个过程叫做 **分词 (Tokenization)**。我们面临一个基础但重要的选择：到底应该把文本切分成多大的单元？

##### 词级分词 (Word-level Tokenization)
最直观的方法是按词语来切分，比如把 "I love LLMs" 分成 `['I', 'love', 'LLMs']`。

* **优点**：简单直观，序列长度短。
* **缺点**：**词表外 (Out-of-Vocabulary, OOV)** 问题。如果模型在训练时没见过 "LLMs" 这个词，它就无法理解。通常模型会把这种未知词替换成一个特殊的 `<UNK>` (Unknown) 标记，这会丢失词语本身的含义。

##### 字节级分词 (Byte-level Tokenization)
为了彻底解决 OOV 问题，我们可以将文本切分到最基础的单元——字节 (byte)。一个字节有 256 种可能的值（从 0 到 255），因此词表大小是固定的 256。任何文本，无论是什么语言或符号，都可以被表示为字节序列。

* **优点**：完全没有 OOV 问题。
* **缺点**：序列变得**极长**。
    * 一个包含 10 个单词的句子，用词级分词可能只有 10 个 token，但用字节级分词可能会变成 50 甚至更多的 token。
    * **拖慢模型训练和推理**：在 Transformer 这类模型中，计算量与序列长度的平方 ($O(n^2)$) 成正比。序列越长，计算成本越高。
    * **学习困难**：序列过长会导致 token 之间的距离被拉大，模型更难捕捉它们之间的**长期依赖关系 (long-term dependencies)**。

---

#### 2. 子词分词：两全其美的方案

**子词分词 (Subword Tokenization)** 是一种介于词级和字节级之间的优雅折中方案。

它的核心思想是：**常见词汇保持为一个完整的 token，而稀有词汇则被拆分成有意义的子词单元。**

子词分词器会用一个比 256 大得多的词表（例如 32,000 或 50,000），来换取对输入文本更好的**压缩率**。例如，如果字节序列 `b'the'` 在我们的训练数据中频繁出现，我们就可以把它作为一个独立的 token 加入词表，从而将一个 3 字节的序列压缩成一个 1 token 的序列。

---

#### 3. 如何找到“子词”？—— 字节对编码 (BPE)

那么，我们如何智能地选择哪些子词单元应该被加入词表呢？答案是使用 **字节对编码 (Byte-Pair Encoding, BPE)** 算法。

BPE 最初是一种数据压缩算法，其工作原理非常简单：**迭代地找出当前数据中最高频的相邻字节对，并将它们合并成一个新的、未被使用过的符号。**

让我们用一个简单的例子来理解这个过程：

假设我们的语料库是： `low low low lower lowest`

1.  **初始化**:
    * 初始词表是所有单个字符：`{'l', 'o', 'w', 'e', 'r', 's', 't'}`。
    * 文本被拆分为字符序列。

2.  **第一轮合并**:
    * 我们发现 `l` 和 `o` 是最频繁出现的相邻对。
    * 我们将 `lo` 合并，并把它作为一个新的子词单元加入词表。
    * 词表变为：`{'l', 'o', 'w', 'e', 'r', 's', 't', 'lo'}`。
    * 语料库现在看起来像： `low low low lower lowest` -> `(lo)w (lo)w (lo)w (lo)wer (lo)west`。

3.  **第二轮合并**:
    * 现在，`lo` 和 `w` 成了最高频的相邻对。
    * 我们合并 `low`，并加入词表。
    * 词表变为：`{'l', ..., 't', 'lo', 'low'}`。
    * 语料库变为：`(low) (low) (low) (low)er (low)est`。

4.  **持续迭代**:
    * 这个过程会一直持续下去，直到词表大小达到我们预设的上限（比如 32,000）。像 `er`、`est` 这样的后缀也可能被合并成子词。

BPE 的目标是通过不断合并高频对来**最大化压缩输入序列**。如果一个单词在语料中足够常见（比如 `low`），它最终会作为一个整体被加入词表。

---

#### 4. 我们要实现的：字节级 BPE 分词器

通过 BPE 算法构建词表的分词器通常被称为 **BPE 分词器**。

在实践中，我们通常实现的是 **字节级 BPE 分词器 (Byte-level BPE Tokenizer)**。这种方法完美地结合了字节级和子词级的优点：

* **初始词表**：从 256 个基本字节开始。
* **训练过程**：在原始文本的字节序列上运行 BPE 算法，不断合并高频字节对，生成更大的子词词表。

这样，我们就得到了一个既能处理任何文本（因为基础是字节）又能有效控制序列长度（因为常见组合被压缩）的强大分词器。

最后，为 BPE 分词器构建词表的过程，我们称之为 **“训练” (training)** 分词器。这与训练神经网络不同，它不涉及梯度下降，而是一个确定性的、基于统计频率的构建过程。

### 2.4 BPE Tokenizer 训练过程 🧠

BPE分词器的训练过程主要包含三个核心步骤。这就像是在教一个完全不懂语言的机器人如何把一篇长长的文章切分成有意义的“词块”。

#### 第一步：词汇表初始化 (Vocabulary Initialization)

**核心思想**：从最基础的单元开始。

我们的目标是训练一个 **字节级 (byte-level)** 的 BPE 分词器，所以我们最初的词汇表就是所有可能的基础字节。

  * 一个字节 (byte) 有 8 位，可以表示 $2^8 = 256$ 种不同的值（从 `0x00` 到 `0xFF`）。
  * 因此，我们的 **初始词汇表大小就是 256**。这个词汇表包含了能构成任何文本的“原子”单元。
  * 词汇表本质上是一个从“词块”（这里是单个字节）到整数ID的映射。

#### 第二步：预分词 (Pre-tokenization)

**核心思想**：为了效率和效果，先对文本进行一次粗略的切分。

理论上，我们可以直接在整个语料库的字节流上统计相邻字节对的频率。但这样做有两个大问题：

1.  **计算成本极高** 😫：每合并一次最高频的字节对，我们就需要重新扫描整个语料库来更新下一轮的频率统计，这太慢了。
2.  **语义边界模糊**：直接在字节流上操作，可能会合并跨越词语边界的字节。例如，`"dog."` 和 `"dog!"` 里的 `"dog"` 后面分别跟着 `"."` 和 `"`。如果不做预分词，模型可能会将 `g` 和 `.` 合并，导致 `"dog"` 这样的核心词汇被破坏，从而让 `"dog."` 和 `"dog!"` 得到完全不同的 token ID，尽管它们的语义高度相似。

**解决方案就是预分词**：

  * 预分词是对语料库进行的一次**粗粒度**的切分，它定义了 BPE 合并操作的**边界**。BPE 的合并永远不会跨越这些边界。
  * 例如，我们可以先把 `"some text that"` 切分成 `['some', ' text', ' that']`。然后，我们只在 `'some'` 内部、`' text'` 内部、`' that'` 内部进行字节对的合并，而不会把 `'e'` (来自 `some`) 和 `' '` (来自`  text `) 合并。
  * **效率提升**：我们可以先统计出每个预分词块（pre-token）在语料中出现的次数。比如 `text` 出现了10次，那么在统计字节对频率时，`('t', 'e')` 的频率就可以直接增加10，而无需遍历语料10次。

**如何进行预分词？**

  * **简单方法**：直接按空格切分 (`s.split(' ')`)，就像原始 BPE 论文中那样。
  * **更精细的方法 (GPT-2/3/4 使用)**：使用正则表达式。原文中提到的 `tiktoken` 库的正则表达式就是一个很好的例子：
    ```python
    PAT = r"""'(?:[sdmt]|ll|ve|re)| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+"""
    ```
    这个复杂的正则是为了更智能地切分文本，它能处理好英语中的缩写 (like `i'll` -\> `i`, `'ll`)、单词、数字以及标点，同时保留空格信息。
    ```python
    >>> import regex as re
    >>> re.findall(PAT, "some text that i'll pre-tokenize")
    # 输出: ['some', ' text', ' that', ' i', "'ll", ' pre', '-', 'tokenize']
    ```
    **注意**：在实际代码中，为了节省内存，应该使用 `re.finditer` 来遍历匹配项，而不是用 `re.findall` 一次性把所有结果存入内存。

-----

#### 第三步：计算 BPE 合并 (Compute BPE Merges)

**核心思想**：贪心地、迭代地合并最高频的相邻字节对。

现在我们有了一堆预分词块和它们的频率，接下来就是 BPE 算法的核心：

1.  **统计频率**：计算所有预分词块中，每一个**相邻**的字节/token对出现的总频率。
2.  **找出最高频**：找到频率最高的那个字节对，例如 `('A', 'B')`。
3.  **合并**：将语料中所有的 `('A', 'B')` 合并成一个新的 token `'AB'`。
4.  **更新词汇表**：将这个新的 token `'AB'` 添加到我们的词汇表中。
5.  **重复**：回到第一步，继续这个过程，直到达到预设的词汇表大小或满足停止条件。

**Tie-breaking (平局处理规则)**：

  * 如果多个字节对的频率并列最高，怎么办？
  * 规则是：选择在**字典序 (lexicographically)** 上更大的那个。
  * 例如，如果 `("A", "B")`, `("A", "C")`, `("B", "ZZ")`, 和 `("BA", "A")` 拥有相同的最高频率，Python中 `max()` 函数会返回 `('BA', 'A')`，因为元组是逐个元素比较大小的。
    ```python
    >>> max([("A", "B"), ("A", "C"), ("B", "ZZ"), ("BA", "A")])
    ('BA', 'A')
    ```
    这是一个确定性的规则，保证了训练过程的可复现性。

-----

#### 特殊 Token (Special Tokens)

在实际应用中，我们还需要一些特殊的控制符号，比如：

  * `<|endoftext|>`: 表示文本结束，模型看到它就知道该停止生成了。
  * `<|pad|>`: 用于将不同长度的序列填充到相同长度。

这些 **特殊 token 应该被视为一个整体，永远不能被 BPE 算法拆分**。因此，在训练开始前，我们会直接把它们加入到词汇表中，并赋予固定的 ID。

-----

### 一个完整的训练例子 🌰

让我们通过原文中的例子来走一遍完整的流程。

**语料库**:

```
low low low low low
lower lower widest widest widest
newest newest newest newest newest newest
```

**特殊 Token**: `<|endoftext|>`

1.  **词汇表初始化**:

      * 我们的词汇表初始包含：`<|endoftext|>` 以及 256 个基础字节 (`b'a'`, `b'b'`, ...)。

2.  **预分词 (假设按空格切分)**:

      * 我们先统计每个“词”的出现频率：
        `{'low': 5, 'lower': 2, 'widest': 3, 'newest': 6}`
      * 为了方便处理，我们将其表示为字节元组和频率的字典：
        `{(b'l', b'o', b'w'): 5, (b'l', b'o', 'w', 'e', 'r'): 2, ...}`

3.  **计算合并**:

      * **第 0 轮 (统计初始频率)**: 我们计算所有相邻字节对的频率。

          * `('l', 'o')`: 在 `low` 中出现5次，`lower` 中出现2次，总共 `5+2=7` 次。
          * `('o', 'w')`: 在 `low` 中出现5次，`lower` 中出现2次，总共 `5+2=7` 次。
          * `('e', 's')`: 在 `widest` 中出现3次，`newest` 中出现6次，总共 `3+6=9` 次。
          * `('s', 't')`: 在 `widest` 中出现3次，`newest` 中出现6次，总共 `3+6=9` 次。
          * ... (计算所有)
          * 我们发现 `('e', 's')` 和 `('s', 't')` 都是最高频（9次）。根据平局规则，`'st'` 在字典序上大于 `'es'`，所以我们选择合并 `('s', 't')`。

      * **第 1 轮合并**: **`s` + `t` -\> `st`**

          * 词汇表新增 `'st'`。
          * 我们的预分词数据现在变为：
            `{(b'l', b'o', b'w'): 5, (b'l', b'o', 'w', 'e', 'r'): 2, (b'w', b'i', b'd', b'e', b'st'): 3, (b'n', b'e', b'w', b'e', b'st'): 6}`

      * **第 2 轮合并**: **`e` + `st` -\> `est`**

          * 现在最高频的相邻对是 `('e', 'st')`，出现了 `3+6=9` 次。
          * 词汇表新增 `'est'`。
          * 数据变为：
            `{(b'l', b'o', 'w'): 5, (b'l', b'o', 'w', 'e', 'r'): 2, (b'w', b'i', b'd', b'est'): 3, (b'n', 'e', 'w', b'est'): 6}`

      * **持续这个过程...**
        原文给出了最终的合并序列（前12次）：`['s t', 'e st', 'o w', 'l ow', 'w est', 'n e', 'ne west', 'w i', 'wi d', 'wid est', 'low e', 'lowe r']`

**如果我们只进行 6 次合并**，那么我们的合并规则就是 `['s t', 'e st', 'o w', 'l ow', 'w est', 'n e']`。

  * **最终词汇表**会包含：

      * `<|endoftext|>`
      * 256 个基础字节
      * `st` (由 `s`+`t` 合并)
      * `est` (由 `e`+`st` 合并)
      * `ow` (由 `o`+`w` 合并)
      * `low` (由 `l`+`ow` 合并)
      * `west` (由 `w`+`est` 合并)
      * `ne` (由 `n`+`e` 合并)

  * **分词应用**:

      * 现在我们用这个训练好的分词器来处理单词 `"newest"`。
      * `newest` -\> `n e w e s t`
      * 应用合并规则：
        1.  `s`+`t` -\> `st`  =\> `n e w e st`
        2.  `e`+`st` -\> `est` =\> `n e w est`
        3.  `w`+`est` -\> `west`=\> `n e west`
        4.  `n`+`e` -\> `ne`  =\> `ne west`
      * 最终，`"newest"` 被分词为 `['ne', 'west']`。